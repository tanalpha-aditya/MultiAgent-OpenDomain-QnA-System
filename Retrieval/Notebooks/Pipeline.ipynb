{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-11-18T16:00:16.420107Z",
     "iopub.status.busy": "2024-11-18T16:00:16.419652Z",
     "iopub.status.idle": "2024-11-18T16:00:43.803105Z",
     "shell.execute_reply": "2024-11-18T16:00:43.801754Z",
     "shell.execute_reply.started": "2024-11-18T16:00:16.420062Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /home/yash/miniconda3/envs/myenv/lib/python3.12/site-packages (4.3.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in /home/yash/miniconda3/envs/myenv/lib/python3.12/site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /home/yash/miniconda3/envs/myenv/lib/python3.12/site-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /home/yash/miniconda3/envs/myenv/lib/python3.12/site-packages (from gensim) (7.0.4)\n",
      "Requirement already satisfied: wrapt in /home/yash/miniconda3/envs/myenv/lib/python3.12/site-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in /home/yash/miniconda3/envs/myenv/lib/python3.12/site-packages (3.1.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.38.0 in /home/yash/miniconda3/envs/myenv/lib/python3.12/site-packages (from sentence-transformers) (4.44.2)\n",
      "Requirement already satisfied: tqdm in /home/yash/miniconda3/envs/myenv/lib/python3.12/site-packages (from sentence-transformers) (4.66.5)\n",
      "Requirement already satisfied: torch>=1.11.0 in /home/yash/miniconda3/envs/myenv/lib/python3.12/site-packages (from sentence-transformers) (2.4.0)\n",
      "Requirement already satisfied: scikit-learn in /home/yash/miniconda3/envs/myenv/lib/python3.12/site-packages (from sentence-transformers) (1.5.1)\n",
      "Requirement already satisfied: scipy in /home/yash/miniconda3/envs/myenv/lib/python3.12/site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.3 in /home/yash/miniconda3/envs/myenv/lib/python3.12/site-packages (from sentence-transformers) (0.24.5)\n",
      "Requirement already satisfied: Pillow in /home/yash/miniconda3/envs/myenv/lib/python3.12/site-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: filelock in /home/yash/miniconda3/envs/myenv/lib/python3.12/site-packages (from huggingface-hub>=0.19.3->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/yash/miniconda3/envs/myenv/lib/python3.12/site-packages (from huggingface-hub>=0.19.3->sentence-transformers) (2024.5.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/yash/miniconda3/envs/myenv/lib/python3.12/site-packages (from huggingface-hub>=0.19.3->sentence-transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/yash/miniconda3/envs/myenv/lib/python3.12/site-packages (from huggingface-hub>=0.19.3->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in /home/yash/miniconda3/envs/myenv/lib/python3.12/site-packages (from huggingface-hub>=0.19.3->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/yash/miniconda3/envs/myenv/lib/python3.12/site-packages (from huggingface-hub>=0.19.3->sentence-transformers) (4.11.0)\n",
      "Requirement already satisfied: sympy in /home/yash/miniconda3/envs/myenv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (1.12)\n",
      "Requirement already satisfied: networkx in /home/yash/miniconda3/envs/myenv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/yash/miniconda3/envs/myenv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /home/yash/miniconda3/envs/myenv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (69.5.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/yash/miniconda3/envs/myenv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/yash/miniconda3/envs/myenv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (2024.7.24)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/yash/miniconda3/envs/myenv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (0.4.4)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/yash/miniconda3/envs/myenv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (0.19.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/yash/miniconda3/envs/myenv/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/yash/miniconda3/envs/myenv/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/yash/miniconda3/envs/myenv/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/yash/miniconda3/envs/myenv/lib/python3.12/site-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/yash/miniconda3/envs/myenv/lib/python3.12/site-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/yash/miniconda3/envs/myenv/lib/python3.12/site-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/yash/miniconda3/envs/myenv/lib/python3.12/site-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (2024.7.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/yash/miniconda3/envs/myenv/lib/python3.12/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rank-bm25\n",
      "  Downloading rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: numpy in /home/yash/miniconda3/envs/myenv/lib/python3.12/site-packages (from rank-bm25) (1.26.4)\n",
      "Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
      "Installing collected packages: rank-bm25\n",
      "Successfully installed rank-bm25-0.2.2\n"
     ]
    }
   ],
   "source": [
    "!pip install rank-bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fpdf\n",
      "  Downloading fpdf-1.7.2.tar.gz (39 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: fpdf\n",
      "  Building wheel for fpdf (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fpdf: filename=fpdf-1.7.2-py2.py3-none-any.whl size=40702 sha256=c5a654586020ff0f1ff6f3a95e800416297cf32aca24f3b1a44f094328cd37f7\n",
      "  Stored in directory: /home/yash/.cache/pip/wheels/6e/62/11/dc73d78e40a218ad52e7451f30166e94491be013a7850b5d75\n",
      "Successfully built fpdf\n",
      "Installing collected packages: fpdf\n",
      "Successfully installed fpdf-1.7.2\n"
     ]
    }
   ],
   "source": [
    "!pip install fpdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyMuPDF\n",
      "  Downloading PyMuPDF-1.24.13-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
      "Downloading PyMuPDF-1.24.13-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (19.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: PyMuPDF\n",
      "Successfully installed PyMuPDF-1.24.13\n"
     ]
    }
   ],
   "source": [
    "!pip install PyMuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yash/miniconda3/envs/myenv/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from nltk import word_tokenize\n",
    "import copy\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import json\n",
    "from gensim.utils import simple_preprocess\n",
    "import joblib\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from rank_bm25 import BM25Okapi\n",
    "from transformers import ViTModel, ViTFeatureExtractor, ViTImageProcessor\n",
    "from PIL import Image\n",
    "import torch\n",
    "from fpdf import FPDF\n",
    "from datetime import datetime\n",
    "import fitz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def get_documents():\n",
    "    ids = []\n",
    "    documents = []\n",
    "    with open('../Dataset.mini_wiki_collection.json', \"r\") as f:\n",
    "        data = json.load(f)\n",
    "        for item in data:\n",
    "            ids.append(item[\"_id\"])\n",
    "            temp = \"\"\n",
    "            for text in item[\"text\"]:\n",
    "                if(text[-1] == '\\n'):\n",
    "                    temp = temp + text[:-1] + \" \"\n",
    "                else:\n",
    "                    temp = temp + text + \" \"\n",
    "            documents.append(temp)\n",
    "    return ids, documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_documents_from_scores(scores):\n",
    "    rankings = []\n",
    "    for score in scores:\n",
    "        rankings.append(score[0])\n",
    "    return rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_tf_idf(documents_tokenized):\n",
    "    # documents_tokenized = get_documents_tokenized(path)\n",
    "    vocab = {}\n",
    "    tf = defaultdict(lambda: {})\n",
    "    freq = defaultdict(lambda: 0)\n",
    "    for i in tqdm(range(len(documents_tokenized))):\n",
    "        tf[i] = defaultdict(lambda: 0)\n",
    "        tokens = documents_tokenized[i]\n",
    "        for token in tokens:\n",
    "            freq[token] += 1\n",
    "            tf[i][token] += 1\n",
    "            if token not in vocab:\n",
    "                vocab[token] = 1\n",
    "\n",
    "    for i in tqdm(range(len(documents_tokenized))):\n",
    "        for token in tf[i].keys():\n",
    "            tf[i][token] = tf[i][token] / len(documents_tokenized[i])\n",
    "    \n",
    "    idf = defaultdict(lambda: 0)\n",
    "    ndoc = defaultdict(lambda: 0)\n",
    "    for i in tqdm(range(len(documents_tokenized))):\n",
    "        temp = defaultdict(lambda: 0)\n",
    "        tokens = documents_tokenized[i]\n",
    "        for token in tokens:\n",
    "            if(temp[token] == 0):\n",
    "                idf[token] += 1\n",
    "                temp[token] += 1\n",
    "\n",
    "    for token in tqdm(idf.keys()):\n",
    "        ndoc[token] = idf[token]\n",
    "        idf[token] = math.log(len(documents_tokenized) / idf[token])\n",
    "\n",
    "    tf_idf = defaultdict(lambda: 0)\n",
    "    for i in tqdm(range(len(documents_tokenized))):\n",
    "        tf_idf[i] = defaultdict(lambda: 0)\n",
    "        for token in documents_tokenized[i]:\n",
    "            tf_idf[i][token] = tf[i][token] * idf[token]\n",
    "    \n",
    "    return tf_idf, idf, ndoc, tf, vocab\n",
    "\n",
    "def get_tf_query(query):\n",
    "    k = len(query)\n",
    "    tf_query = defaultdict(lambda: 0)\n",
    "    for i in range(k):\n",
    "        tf_query[query[i]] += 1\n",
    "    for token in tf_query.keys():\n",
    "        tf_query[token] /= k\n",
    "    return tf_query\n",
    "\n",
    "def get_tf_idf_query(query, idf_dict):\n",
    "    query = simple_preprocess(query)\n",
    "    tf_idf_query = defaultdict(lambda: 0)\n",
    "    tf_query = get_tf_query(query)\n",
    "    for token in tf_query.keys():\n",
    "        tf_idf_query[token] = tf_query[token] * idf_dict[token]\n",
    "    return tf_idf_query\n",
    "    \n",
    "def get_tf_idf_vector(tf_idf_instance, vocab):\n",
    "    temp = []\n",
    "    for key in vocab.keys():\n",
    "        temp.append(tf_idf_instance[key])\n",
    "    return temp\n",
    "    \n",
    "def cosine_similarity(v1, v2):\n",
    "    v1 = np.array(v1)\n",
    "    v2 = np.array(v2)\n",
    "    if(np.linalg.norm(v1) != 0 and np.linalg.norm(v2) != 0):\n",
    "        sim = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "    else:\n",
    "        sim = 0\n",
    "    return sim\n",
    "\n",
    "def print_non_zero(dict):\n",
    "    for key in dict.keys():\n",
    "        if(dict[key] > 0):\n",
    "            print(dict[key])\n",
    "\n",
    "def get_document_vectors(tf_idf_dict, vocab):\n",
    "    document_vectors = []\n",
    "    for i in tqdm(range(len(list(tf_idf_dict.keys())))):\n",
    "        document_vector = get_tf_idf_vector(tf_idf_dict[i], vocab)\n",
    "        document_vectors.append(document_vector)\n",
    "    return document_vectors\n",
    "\n",
    "\n",
    "def tf_idf_rankings(query, idf_dict, tf_idf_dict, vocab, document_matrix, k):\n",
    "    query_vector = np.reshape(np.array(get_tf_idf_vector(get_tf_idf_query(query, idf_dict), vocab)), (1, -1))\n",
    "    scores = []\n",
    "    dot_products = document_matrix @ query_vector.T\n",
    "\n",
    "    query_norm = np.linalg.norm(query_vector)\n",
    "    doc_norms = np.linalg.norm(document_matrix, axis=1, keepdims=True)\n",
    "    cosine_similarities = dot_products / (doc_norms * query_norm)\n",
    "    cosine_similarities = cosine_similarities.flatten()\n",
    "    rankings = np.argsort(cosine_similarities)[::-1]\n",
    "    rankings = rankings[:k]\n",
    "    scores = []\n",
    "    for rank in rankings:\n",
    "        scores.append(cosine_similarities[rank])\n",
    "    # scores = sorted(cosine_similarities, key=lambda x: x[1], reverse=True)\n",
    "    # scores = scores[:k]\n",
    "    # rankings = get_documents_from_scores(scores)\n",
    "    return rankings, scores\n",
    "\n",
    "def tf_idf_pipeline(query, idf_dict_path, tf_idf_dict_path, vocab_path, document_matrix_path, ids_path, k):\n",
    "    idf_dict = joblib.load(idf_dict_path)\n",
    "    print(\"idf loaded...\")\n",
    "    tf_idf_dict = joblib.load(tf_idf_dict_path)\n",
    "    print(\"tf-idf loaded...\")\n",
    "    vocab = joblib.load(vocab_path)\n",
    "    print(\"vocab loaded...\")\n",
    "    document_matrix = joblib.load(document_matrix_path)\n",
    "    print(\"document_matrix loaded...\")\n",
    "    ids = joblib.load(ids_path)\n",
    "    print(\"ids loaded\")\n",
    "    rankings, scores = tf_idf_rankings(query, idf_dict, tf_idf_dict, vocab, document_matrix, k)\n",
    "    rankings2 = []\n",
    "    for ranking in tqdm(rankings):\n",
    "        rankings2.append(ids[ranking])\n",
    "    return rankings2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tf_idf_pipeline() missing 6 required positional arguments: 'idf_dict_path', 'tf_idf_dict_path', 'vocab_path', 'document_matrix_path', 'ids_path', and 'k'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtf_idf_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mIn the United States, why are positions like Attorney General, Secretary of State, etc. appointed by the president at the federal level but elected by the people at the state level? Had it ever been proposed to do this differently?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: tf_idf_pipeline() missing 6 required positional arguments: 'idf_dict_path', 'tf_idf_dict_path', 'vocab_path', 'document_matrix_path', 'ids_path', and 'k'"
     ]
    }
   ],
   "source": [
    "tf_idf_pipeline(\"In the United States, why are positions like Attorney General, Secretary of State, etc. appointed by the president at the federal level but elected by the people at the state level? Had it ever been proposed to do this differently?\", 'idf.pkl', 'tf_idf_dict.pkl', 'vocab.pkl', 'document_matrix.pkl', 'ids.pkl', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yash/miniconda3/envs/myenv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_open_source_embeddings(documents):\n",
    "    documents_embeddings = []\n",
    "    for document in tqdm(documents):\n",
    "        documents_embeddings.append(model.encode(document))\n",
    "    return documents_embeddings\n",
    "    \n",
    "def open_source_rankings(query, document_embeddings, k):\n",
    "    query_embedding = model.encode(query)\n",
    "    scores = []\n",
    "    for idx, embedding in enumerate(document_embeddings):\n",
    "        scores.append((idx, cosine_similarity(query_embedding, embedding)))\n",
    "    scores = sorted(scores, key=lambda x: x[1], reverse=True)\n",
    "    scores = scores[:k]\n",
    "    rankings = get_documents_from_scores(scores)\n",
    "    return rankings, scores\n",
    "def open_source_pipeline(query, documents_embeddings_path, ids_path, k):\n",
    "    document_embeddings = joblib.load(documents_embeddings_path)\n",
    "    ids = joblib.load(ids_path)\n",
    "    rankings, scores = open_source_rankings(query, document_embeddings, k)\n",
    "    rankings2 = []\n",
    "    for ranking in tqdm(rankings):\n",
    "        rankings2.append(ids[ranking])\n",
    "    return rankings2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 93622.86it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['54376', '2015257', '31739', '24113', '31187327']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open_source_pipeline(\"In the United States, why are positions like Attorney General, Secretary of State, etc. appointed by the president at the federal level but elected by the people at the state level? Had it ever been proposed to do this differently?\", 'open_source_embeddings.pkl', 'ids.pkl', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def bm25_pipeline(query, bm25_path, ids_path, k):\n",
    "    bm25 = joblib.load(bm25_path)\n",
    "    ids = joblib.load(ids_path)\n",
    "    ranking = bm25.get_scores(simple_preprocess(query))\n",
    "    ranking = np.argsort(np.array(ranking))[::-1]\n",
    "    ranking = ranking[:k]\n",
    "    for j in range(len(ranking)):\n",
    "        ranking[j] = ids[ranking[j]]\n",
    "    return ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 260962,  232530, 3414021,   54376, 2015257])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25_pipeline(\"In the United States, why are positions like Attorney General, Secretary of State, etc. appointed by the president at the federal level but elected by the people at the state level? Had it ever been proposed to do this differently?\", 'bm25-1_0.pkl', 'ids.pkl', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def pdf_to_image(pdf_path, zoom=2.0):\n",
    "    # Open the PDF file\n",
    "    pdf_document = fitz.open(pdf_path)\n",
    "    \n",
    "    # Create a list to store image paths\n",
    "    image_paths = []\n",
    "    \n",
    "    # Create an 'Images' directory if it doesn't exist\n",
    "    os.makedirs(\"Images\", exist_ok=True)\n",
    "    \n",
    "    # Iterate over PDF pages and convert each to an image\n",
    "    for page_num in range(len(pdf_document)):\n",
    "        page = pdf_document.load_page(page_num)  # Load the page\n",
    "        \n",
    "        # Set zoom level to improve quality\n",
    "        mat = fitz.Matrix(zoom, zoom)  # Create a transformation matrix with the zoom level\n",
    "        pix = page.get_pixmap(matrix=mat)  # Render the page to an image with the specified zoom\n",
    "        \n",
    "        image_file = f'Images/{os.path.basename(pdf_path)}_page_{page_num}.png'\n",
    "        pix.save(image_file)  # Save the image as PNG\n",
    "        image_paths.append(image_file)\n",
    "    \n",
    "    # Return the list containing paths of all images\n",
    "    return image_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def create_pdf(input_text):\n",
    "    # Create instance of FPDF class\n",
    "    pdf = FPDF()\n",
    "    \n",
    "    # Add a page\n",
    "    pdf.add_page()\n",
    "    \n",
    "    # Set font\n",
    "    pdf.set_font(\"Arial\", size=10)\n",
    "    \n",
    "    # Split the input text into multiple lines if necessary\n",
    "    # This ensures that the text fits the page and multiple pages are handled\n",
    "    pdf.multi_cell(0, 5, txt=input_text)\n",
    "    \n",
    "    # Create a unique file name with the current time\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    file_name = f\"PDFs/Aditya_{timestamp}.pdf\"\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(file_name), exist_ok=True)\n",
    "    \n",
    "    # Save the PDF\n",
    "    pdf.output(file_name)\n",
    "    \n",
    "    # Return the file path\n",
    "    return file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def sanitize_text(text):\n",
    "    \"\"\"\n",
    "    Cleans and standardizes text by keeping only alphanumeric characters and spaces.\n",
    "    Args:\n",
    "        text (str): Text to sanitize.\n",
    "    Returns:\n",
    "        str: Sanitized text.\n",
    "    \"\"\"\n",
    "    if isinstance(text, str):\n",
    "        # Use regex to keep only alphanumeric characters and spaces\n",
    "        text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "        # Optionally, collapse multiple spaces into a single space\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def text_to_images(text):\n",
    "    text = sanitize_text(text)\n",
    "    pdf_path = create_pdf(text)\n",
    "    image_paths = pdf_to_image(pdf_path)\n",
    "    return image_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def documents_to_images(path):\n",
    "    document_set = []\n",
    "    for filename in os.listdir(path):\n",
    "        file_path = os.path.join(path, filename)\n",
    "        if os.path.isfile(file_path):\n",
    "            with open(file_path, \"r\") as f:\n",
    "                content = f.read()\n",
    "                document_set.append(content)\n",
    "    document_image_paths = []\n",
    "    for document in document_set:\n",
    "        image_paths = text_to_images(document)\n",
    "        document_image_paths.append(image_paths)\n",
    "    return document_image_paths\n",
    "\n",
    "def single_unit_embedding(text):\n",
    "    image_paths = text_to_images(text)\n",
    "    temp = []\n",
    "    for image_path in image_paths:\n",
    "        image = Image.open(image_path)\n",
    "        inputs = processor(images=image, return_tensors=\"pt\")\n",
    "        outputs = model(**inputs)\n",
    "        vector = outputs.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "        temp.append(vector)\n",
    "    return np.mean(np.array(temp), axis=0)\n",
    "\n",
    "def single_image_embedding(image):\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    vector = outputs.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "    return vector\n",
    "\n",
    "def documents_to_vision_embeddings(documents):\n",
    "    document_vision_embeddings = []\n",
    "    for document in tqdm(documents):\n",
    "        vector = single_unit_embedding(document)\n",
    "        document_vision_embeddings.append(vector)\n",
    "    return document_vision_embeddings\n",
    "\n",
    "def queries_to_vision_embeddings(queries):\n",
    "    query_vision_embeddings = []\n",
    "    for query in tqdm(queries):\n",
    "        vector = single_unit_embedding(query)\n",
    "        query_vision_embeddings.append(vector)\n",
    "    return query_vision_embeddings\n",
    "\n",
    "def vision_rankings(query_embedding, document_embeddings, k):\n",
    "    # query_embedding = single_unit_embedding(query)\n",
    "    scores = []\n",
    "    for idx, embedding in enumerate(document_embeddings):\n",
    "        scores.append((idx, cosine_similarity(query_embedding[0], embedding[0])))\n",
    "    scores = sorted(scores, key=lambda x: x[1], reverse=True)\n",
    "    scores = scores[:k]\n",
    "    rankings = get_documents_from_scores(scores)\n",
    "    return rankings, scores\n",
    "\n",
    "def vision_pipeline(query, document_embeddings_path, ids_path, k):\n",
    "    # document_embeddings = joblib.load(document_embeddings_path)\n",
    "    ids = joblib.load(ids_path)\n",
    "    documents_vision_embeddings2 = []\n",
    "    with open(\"document-vision-embeddings.json\", \"r\") as f:\n",
    "        document_vision_embeddings2 = json.load(f)\n",
    "    document_vision_embeddings = []\n",
    "    for embedding in tqdm(document_vision_embeddings2):\n",
    "        document_vision_embeddings.append(np.array(embedding))\n",
    "    print(\"loaded embeddings\")\n",
    "    query_embedding = single_unit_embedding(query)\n",
    "    rankings, scores = vision_rankings(query_embedding, document_vision_embeddings, k)\n",
    "    rankings2 = []\n",
    "    for ranking in rankings:\n",
    "        rankings2.append(ids[ranking])\n",
    "    return rankings2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 35600.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded embeddings\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['48272667', '53470812', '50170741', '19865700', '39790870']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vision_pipeline(\"In the United States, why are positions like Attorney General, Secretary of State, etc. appointed by the president at the federal level but elected by the people at the state level? Had it ever been proposed to do this differently?\", 'document-vision-embeddings.json', 'ids.pkl', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
