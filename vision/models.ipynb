{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9509306,"sourceType":"datasetVersion","datasetId":5788126},{"sourceId":9509325,"sourceType":"datasetVersion","datasetId":5788139}],"dockerImageVersionId":30776,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install git+https://github.com/huggingface/transformers","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-29T12:03:15.573787Z","iopub.execute_input":"2024-09-29T12:03:15.574161Z","iopub.status.idle":"2024-09-29T12:04:03.517294Z","shell.execute_reply.started":"2024-09-29T12:03:15.574123Z","shell.execute_reply":"2024-09-29T12:04:03.516071Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Collecting git+https://github.com/huggingface/transformers\n  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-yfk9atq1\n  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-yfk9atq1\n  Resolved https://github.com/huggingface/transformers to commit 2e24ee4dfa39cc0bc264b89edbccc373c8337086\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.46.0.dev0) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers==4.46.0.dev0) (0.25.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.46.0.dev0) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.46.0.dev0) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.46.0.dev0) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.46.0.dev0) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.46.0.dev0) (2.32.3)\nCollecting tokenizers<0.21,>=0.20 (from transformers==4.46.0.dev0)\n  Downloading tokenizers-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.46.0.dev0) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.46.0.dev0) (4.66.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.46.0.dev0) (2024.6.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.46.0.dev0) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.46.0.dev0) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.46.0.dev0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.46.0.dev0) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.46.0.dev0) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.46.0.dev0) (2024.8.30)\nDownloading tokenizers-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: transformers\n  Building wheel for transformers (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for transformers: filename=transformers-4.46.0.dev0-py3-none-any.whl size=9922875 sha256=f1f55f0b6a8bbb6edf0a07fd84b935374db0f1aa1d7ef442be6eb19e2949490f\n  Stored in directory: /tmp/pip-ephem-wheel-cache-jkcrgknj/wheels/c0/14/d6/6c9a5582d2ac191ec0a483be151a4495fe1eb2a6706ca49f1b\nSuccessfully built transformers\nInstalling collected packages: tokenizers, transformers\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.19.1\n    Uninstalling tokenizers-0.19.1:\n      Successfully uninstalled tokenizers-0.19.1\nSuccessfully installed tokenizers-0.20.0 transformers-4.46.0.dev0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip uninstall -y transformers","metadata":{"execution":{"iopub.status.busy":"2024-09-29T12:02:50.941869Z","iopub.execute_input":"2024-09-29T12:02:50.942636Z","iopub.status.idle":"2024-09-29T12:03:00.202896Z","shell.execute_reply.started":"2024-09-29T12:02:50.942594Z","shell.execute_reply":"2024-09-29T12:03:00.201896Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Found existing installation: transformers 4.44.2\nUninstalling transformers-4.44.2:\n  Successfully uninstalled transformers-4.44.2\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install qwen-vl-utils","metadata":{"execution":{"iopub.status.busy":"2024-09-29T12:04:21.832096Z","iopub.execute_input":"2024-09-29T12:04:21.832528Z","iopub.status.idle":"2024-09-29T12:04:35.124322Z","shell.execute_reply.started":"2024-09-29T12:04:21.832484Z","shell.execute_reply":"2024-09-29T12:04:35.123184Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Collecting qwen-vl-utils\n  Downloading qwen_vl_utils-0.0.8-py3-none-any.whl.metadata (3.6 kB)\nCollecting av (from qwen-vl-utils)\n  Downloading av-13.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from qwen-vl-utils) (21.3)\nRequirement already satisfied: pillow in /opt/conda/lib/python3.10/site-packages (from qwen-vl-utils) (10.3.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from qwen-vl-utils) (2.32.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->qwen-vl-utils) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->qwen-vl-utils) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->qwen-vl-utils) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->qwen-vl-utils) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->qwen-vl-utils) (2024.8.30)\nDownloading qwen_vl_utils-0.0.8-py3-none-any.whl (5.9 kB)\nDownloading av-13.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.0/33.0 MB\u001b[0m \u001b[31m52.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: av, qwen-vl-utils\nSuccessfully installed av-13.0.0 qwen-vl-utils-0.0.8\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor","metadata":{"execution":{"iopub.status.busy":"2024-09-29T12:04:44.675076Z","iopub.execute_input":"2024-09-29T12:04:44.675979Z","iopub.status.idle":"2024-09-29T12:05:06.250731Z","shell.execute_reply.started":"2024-09-29T12:04:44.675932Z","shell.execute_reply":"2024-09-29T12:05:06.249902Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"import torch\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"","metadata":{"execution":{"iopub.status.busy":"2024-09-29T12:01:50.225828Z","iopub.execute_input":"2024-09-29T12:01:50.226112Z","iopub.status.idle":"2024-09-29T12:01:55.909800Z","shell.execute_reply.started":"2024-09-29T12:01:50.226079Z","shell.execute_reply":"2024-09-29T12:01:55.908794Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"device","metadata":{"execution":{"iopub.status.busy":"2024-09-29T12:02:41.823664Z","iopub.execute_input":"2024-09-29T12:02:41.824127Z","iopub.status.idle":"2024-09-29T12:02:41.831180Z","shell.execute_reply.started":"2024-09-29T12:02:41.824093Z","shell.execute_reply":"2024-09-29T12:02:41.830258Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"'cuda'"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\nfrom qwen_vl_utils import process_vision_info\n\n# default: Load the model on the available device(s)\nmodel = Qwen2VLForConditionalGeneration.from_pretrained(\n    \"Qwen/Qwen2-VL-2B-Instruct\", torch_dtype=\"auto\", device_map=\"auto\"\n)\n\n# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n# model = Qwen2VLForConditionalGeneration.from_pretrained(\n#     \"Qwen/Qwen2-VL-2B-Instruct\",\n#     torch_dtype=torch.bfloat16,\n#     attn_implementation=\"flash_attention_2\",\n#     device_map=\"auto\",\n# )\n\n# default processer\nprocessor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n\n# The default range for the number of visual tokens per image in the model is 4-16384. You can set min_pixels and max_pixels according to your needs, such as a token count range of 256-1280, to balance speed and memory usage.\n# min_pixels = 256*28*28\n# max_pixels = 1280*28*28\n# processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\", min_pixels=min_pixels, max_pixels=max_pixels)\n\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"image\",\n                \"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\",\n            },\n            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n        ],\n    }\n]\n\n# Preparation for inference\ntext = processor.apply_chat_template(\n    messages, tokenize=False, add_generation_prompt=True\n)\nimage_inputs, video_inputs = process_vision_info(messages)\ninputs = processor(\n    text=[text],\n    images=image_inputs,\n    videos=video_inputs,\n    padding=True,\n    return_tensors=\"pt\",\n)\ninputs = inputs.to(\"cuda\")\n\n# Inference: Generation of the output\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\n    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\n    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_text)","metadata":{"execution":{"iopub.status.busy":"2024-09-29T12:05:11.619974Z","iopub.execute_input":"2024-09-29T12:05:11.620679Z","iopub.status.idle":"2024-09-29T12:05:43.429203Z","shell.execute_reply.started":"2024-09-29T12:05:11.620641Z","shell.execute_reply":"2024-09-29T12:05:43.427499Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b32cf8fef584f939c06e96c065ae199"}},"metadata":{}},{"name":"stderr","text":"Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/56.4k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9c840375d024ec7845f244130d0d0bd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1135be294bcd43f19312cdd980aacbda"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/3.99G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b664083989bb4d0a8fa6834396906a47"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/429M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c8982efd84ab4b34bc98b505fca72795"}},"metadata":{}},{"name":"stderr","text":"`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d18af5060fc7413f99ef331c66fb828c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/272 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa5c796cb26e48079ee90ac6b4c8b89f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/347 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c70405e11b4a4672877de06b5ec0c905"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/4.19k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84bc67e8b0fa47ed9ee0812e7b406d8b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e743c391417e428aa94a7720417522d5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"571759967281474496864782ffdfbff0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10c33c4cd9004edda6742363c1cdccce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"chat_template.json:   0%|          | 0.00/1.05k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48305dbd6f1144b2b515cbf55587ea65"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","Cell \u001b[0;32mIn[7], line 53\u001b[0m\n\u001b[1;32m     50\u001b[0m inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Inference: Generation of the output\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m generated_ids \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m generated_ids_trimmed \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     55\u001b[0m     out_ids[\u001b[38;5;28mlen\u001b[39m(in_ids) :] \u001b[38;5;28;01mfor\u001b[39;00m in_ids, out_ids \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(inputs\u001b[38;5;241m.\u001b[39minput_ids, generated_ids)\n\u001b[1;32m     56\u001b[0m ]\n\u001b[1;32m     57\u001b[0m output_text \u001b[38;5;241m=\u001b[39m processor\u001b[38;5;241m.\u001b[39mbatch_decode(\n\u001b[1;32m     58\u001b[0m     generated_ids_trimmed, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, clean_up_tokenization_spaces\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     59\u001b[0m )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:2048\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2040\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2041\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2042\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2043\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2044\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2045\u001b[0m     )\n\u001b[1;32m   2047\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2048\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2049\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2050\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2051\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2052\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2053\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2054\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2055\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2056\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2058\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2059\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2060\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2061\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2062\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2067\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2068\u001b[0m     )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:3008\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3005\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[1;32m   3007\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 3008\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3010\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   3011\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/qwen2_vl/modeling_qwen2_vl.py:1691\u001b[0m, in \u001b[0;36mQwen2VLForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, pixel_values, pixel_values_videos, image_grid_thw, video_grid_thw, rope_deltas)\u001b[0m\n\u001b[1;32m   1689\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pixel_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1690\u001b[0m     pixel_values \u001b[38;5;241m=\u001b[39m pixel_values\u001b[38;5;241m.\u001b[39mtype(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvisual\u001b[38;5;241m.\u001b[39mget_dtype())\n\u001b[0;32m-> 1691\u001b[0m     image_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisual\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrid_thw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_grid_thw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1692\u001b[0m     image_mask \u001b[38;5;241m=\u001b[39m (input_ids \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mimage_token_id)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mexpand_as(inputs_embeds)\n\u001b[1;32m   1693\u001b[0m     image_embeds \u001b[38;5;241m=\u001b[39m image_embeds\u001b[38;5;241m.\u001b[39mto(inputs_embeds\u001b[38;5;241m.\u001b[39mdevice, inputs_embeds\u001b[38;5;241m.\u001b[39mdtype)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/qwen2_vl/modeling_qwen2_vl.py:1128\u001b[0m, in \u001b[0;36mQwen2VisionTransformerPretrainedModel.forward\u001b[0;34m(self, hidden_states, grid_thw)\u001b[0m\n\u001b[1;32m   1125\u001b[0m cu_seqlens \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mpad(cu_seqlens, (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[0;32m-> 1128\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mblk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcu_seqlens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcu_seqlens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrotary_pos_emb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrotary_pos_emb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmerger(hidden_states)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/qwen2_vl/modeling_qwen2_vl.py:431\u001b[0m, in \u001b[0;36mQwen2VLVisionBlock.forward\u001b[0;34m(self, hidden_states, cu_seqlens, rotary_pos_emb)\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states, cu_seqlens, rotary_pos_emb) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 431\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcu_seqlens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcu_seqlens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrotary_pos_emb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrotary_pos_emb\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    434\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(hidden_states))\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/qwen2_vl/modeling_qwen2_vl.py:404\u001b[0m, in \u001b[0;36mVisionSdpaAttention.forward\u001b[0;34m(self, hidden_states, cu_seqlens, rotary_pos_emb)\u001b[0m\n\u001b[1;32m    402\u001b[0m k \u001b[38;5;241m=\u001b[39m k\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    403\u001b[0m v \u001b[38;5;241m=\u001b[39m v\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 404\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    405\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    406\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mreshape(seq_length, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 6.10 GiB. GPU 0 has a total capacity of 14.74 GiB of which 5.08 GiB is free. Process 2689 has 9.66 GiB memory in use. Of the allocated memory 9.35 GiB is allocated by PyTorch, and 189.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 6.10 GiB. GPU 0 has a total capacity of 14.74 GiB of which 5.08 GiB is free. Process 2689 has 9.66 GiB memory in use. Of the allocated memory 9.35 GiB is allocated by PyTorch, and 189.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","output_type":"error"}]},{"cell_type":"code","source":"pip install transformers","metadata":{"execution":{"iopub.status.busy":"2024-09-29T14:00:20.699903Z","iopub.execute_input":"2024-09-29T14:00:20.700413Z","iopub.status.idle":"2024-09-29T14:00:34.989334Z","shell.execute_reply.started":"2024-09-29T14:00:20.700377Z","shell.execute_reply":"2024-09-29T14:00:34.988128Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.44.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.25.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import ViTModel, ViTFeatureExtractor, ViTImageProcessor\nfrom PIL import Image\nimport torch","metadata":{"execution":{"iopub.status.busy":"2024-09-29T15:00:52.240216Z","iopub.execute_input":"2024-09-29T15:00:52.240818Z","iopub.status.idle":"2024-09-29T15:01:09.695659Z","shell.execute_reply.started":"2024-09-29T15:00:52.240778Z","shell.execute_reply":"2024-09-29T15:01:09.694683Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import numpy as np","metadata":{"execution":{"iopub.status.busy":"2024-09-29T14:06:08.600546Z","iopub.execute_input":"2024-09-29T14:06:08.601144Z","iopub.status.idle":"2024-09-29T14:06:08.605263Z","shell.execute_reply.started":"2024-09-29T14:06:08.601093Z","shell.execute_reply":"2024-09-29T14:06:08.604342Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"","metadata":{"execution":{"iopub.status.busy":"2024-09-29T14:02:06.245786Z","iopub.execute_input":"2024-09-29T14:02:06.246855Z","iopub.status.idle":"2024-09-29T14:02:06.310451Z","shell.execute_reply.started":"2024-09-29T14:02:06.246814Z","shell.execute_reply":"2024-09-29T14:02:06.309215Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"image = Image.open('/kaggle/input/images2/IMG1.jpg')\nprint(type(np.array(image)))\nprint(np.array(image).shape)","metadata":{"execution":{"iopub.status.busy":"2024-09-29T14:07:48.421549Z","iopub.execute_input":"2024-09-29T14:07:48.421942Z","iopub.status.idle":"2024-09-29T14:07:48.455390Z","shell.execute_reply.started":"2024-09-29T14:07:48.421902Z","shell.execute_reply":"2024-09-29T14:07:48.454531Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"<class 'numpy.ndarray'>\n(614, 614, 3)\n","output_type":"stream"}]},{"cell_type":"code","source":"model = ViTModel.from_pretrained('google/vit-base-patch16-224')\nfeature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224')\n\nimage = Image.open('/kaggle/input/images2/IMG1.jpg')\ninputs = feature_extractor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\nvector = outputs.last_hidden_state.mean(dim=1).detach().numpy()  # Image embedding","metadata":{"execution":{"iopub.status.busy":"2024-09-29T14:07:58.987958Z","iopub.execute_input":"2024-09-29T14:07:58.988702Z","iopub.status.idle":"2024-09-29T14:07:59.953175Z","shell.execute_reply.started":"2024-09-29T14:07:58.988660Z","shell.execute_reply":"2024-09-29T14:07:59.952188Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"# Load the model and image processor\nmodel = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\nprocessor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')\n\n# Open an image and ensure it's RGB\nimage = Image.open('/kaggle/input/images2/IMG1.jpg')\n\n# Preprocess the image\ninputs = processor(images=image, return_tensors=\"pt\")\n\n# Extract image features\noutputs = model(**inputs)\nvector = outputs.last_hidden_state.mean(dim=1).detach().numpy()\n\nprint(vector)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-29T14:12:09.732202Z","iopub.execute_input":"2024-09-29T14:12:09.733029Z","iopub.status.idle":"2024-09-29T14:12:16.379557Z","shell.execute_reply.started":"2024-09-29T14:12:09.732988Z","shell.execute_reply":"2024-09-29T14:12:16.378478Z"},"trusted":true},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/502 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1004ece469984d2aa2b0ccc7f908848c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d82f74eb02d4f66ae29a56290f4831b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0fc3c6d33553457f9007dbcb5c224528"}},"metadata":{}},{"name":"stdout","text":"[[-0.06432847  0.0523778   0.18656945  0.20612575 -0.10840616  0.09002342\n   0.09847436  0.10438419  0.01100009  0.10592143 -0.06610611 -0.09910979\n  -0.15100439  0.09057885  0.12569845  0.03268489 -0.20132649  0.04355562\n  -0.06982531 -0.05790578 -0.09743076  0.02509805 -0.14576586  0.08895053\n  -0.05265562  0.03161643 -0.19529468  0.0626617  -0.09965155 -0.29001278\n  -0.14507376  0.22557437  0.08791335  0.00177897  0.03332391  0.18605205\n  -0.13825248  0.09435226 -0.08593917 -0.12930559 -0.19642825 -0.08429247\n  -0.24382956  0.17645566  0.20983358  0.01682295  0.09139233 -0.15736204\n  -0.18509856  0.00943302  0.12606984  0.10259368 -0.31681657 -0.14263792\n   0.0769181  -0.0265936  -0.20542534 -0.37944072 -0.21081628 -0.32027236\n   0.19711845 -0.24671835 -0.10781609 -0.0421334   0.09764279 -0.18435186\n  -0.12910953 -0.00065595 -0.01356702  0.14996903 -0.0297479   0.01884507\n  -0.04180967 -0.0376329   0.19309418  0.28000805 -0.21920285  0.06154418\n   0.15962468 -0.17970555  0.13335499 -0.10628784 -0.35024807  0.07094605\n  -0.10197631 -0.01857885 -0.05003259 -0.01767959  0.02831404  0.13889976\n  -0.03389669  0.04736297  0.07568961  0.10154848 -0.10630504  0.03712185\n  -0.05470812  0.41744828 -0.05879864  0.07926156  0.13816667  0.12452222\n  -0.1369726  -0.07974948 -0.29305407  0.02812704 -0.15421094 -0.02350654\n   0.10965048  0.1051292  -0.13891165 -0.00478786 -0.12035431  0.09331755\n   0.00789609 -0.34213093 -0.04534742 -0.00383297  0.05356692  0.02056856\n  -0.00351052 -0.28056514  0.02510083  0.13158274  0.08227833 -0.06775136\n   0.20655693  0.2203024   0.0864305   0.11391238 -0.20956986 -0.0817073\n  -0.08844144 -0.1018543  -0.09912393  0.1612125   0.00840576 -0.11600388\n  -0.09446     0.06088964  0.07027248  0.06297798  0.17087883 -0.12033185\n   0.11742304 -0.01069787 -0.12826546  0.28004065  0.03363292 -0.10532638\n  -0.17087276  0.20382488  0.08891542  0.13226733  0.09704718 -0.01988248\n   0.0451275  -0.12420041 -0.01106455 -0.09521715 -0.07115933 -0.10205433\n  -0.10274393  0.00257032  0.10642621  0.11320874  0.0047122   0.13117424\n   0.09671428 -0.13075459 -0.08461238  0.05508508 -0.01741658 -0.14983709\n   0.01803    -0.01894014  0.07085294 -0.1807143   0.15507776  0.00704488\n  -0.0356687   0.0210873  -0.10736989 -0.03427493 -0.05081833 -0.10098071\n  -0.01375256 -0.06221746 -0.12037914  0.02369847  0.11312371  0.14743201\n   0.04762987 -0.07595706 -0.08354051  0.07268675 -0.1394199  -0.33763176\n   0.0463762  -0.01410738  0.20378706  0.14534605 -0.01691866  0.05997026\n  -0.03056636  0.12970543  0.11256119  0.0385292   0.13925374  0.08779635\n  -0.05368328  0.08698618 -0.2059121  -0.1084406   0.16910431 -0.23096362\n  -0.3831338  -0.18153124 -0.21206805 -0.14716427 -0.13390179  0.05426643\n   0.03161515 -0.04211213 -0.17975192 -0.17925665  0.01704998 -0.11303902\n  -0.20658097 -0.20034473  0.0029306   0.00334565 -0.01698459  0.03957252\n   0.07896514 -0.15382202 -0.01131241  0.19689901  0.15051809 -0.18914752\n  -0.1430554   0.19030096  0.02191344  0.04924811 -0.16910063 -0.17053486\n   0.00475024 -0.25389886  0.13356921 -0.05409703  0.06255363  0.00650585\n   0.24536145 -0.03289167  0.19397919  0.19576378  0.16185834  0.08428852\n  -0.088365   -0.07379187  0.13935594 -0.08053098 -0.04233551  0.04253151\n  -0.01220322  0.0365898   0.14236346  0.11076217  0.03326692  0.06291043\n  -0.14266996  0.10834674  0.35236356 -0.04663308  0.38383475  0.01977774\n  -0.09075964 -0.05315976  0.1318304  -0.1435043  -0.16525339 -0.03975493\n  -0.00612167  0.07742944 -0.06509111  0.11399769 -0.00448077 -0.0993759\n  -0.07046439  0.16164549  0.11013795 -0.25720492 -0.20755717 -0.01459652\n  -0.20990126  0.20656995  0.11420354  0.11711008 -0.01541326 -0.10104331\n  -0.09941576 -0.0144002  -0.2787414  -0.06592549 -0.06903749  0.19519821\n   0.04356798 -0.18946992 -0.02449235 -0.09936507 -0.06430101 -0.3206109\n  -0.12394284 -0.04112244 -0.30829144  0.17156008 -0.05874532  0.19581094\n  -0.09293944  0.03972201 -0.01260176  0.0090156  -0.16001877 -0.09115949\n  -0.06532125  0.3094488  -0.3484669  -0.07356977  0.05377865 -0.08655757\n   0.03700785 -0.24327344 -0.02296246 -0.08414929  0.24529305  0.15847163\n  -0.16871157  0.1264177   0.04880324 -0.06248526 -0.01268301 -0.00898379\n   0.16544095  0.13175653  0.00151087  0.09854244  0.06401119 -0.19600026\n   0.22679062 -0.08529954  0.17929606  0.04131312 -0.17163798  0.10628835\n  -0.01809741  0.06283738 -0.08007808  0.00954759  0.09688674  0.04713583\n  -0.12763862 -0.18122442 -0.02985151  0.0161969   0.05332337 -0.11684036\n  -0.28838992  0.12923315  0.30682957  0.09126832 -0.0237939  -0.03178692\n   0.19584106 -0.2860531   0.13328288 -0.05860995  0.25914764  0.00573228\n  -0.33244184 -0.309577   -0.01641439 -0.22674018 -0.04875729  0.0214275\n   0.03922661 -0.27659494 -0.06242066 -0.05949214 -0.03416923  0.16973685\n  -0.01180386 -0.07942505 -0.03886724 -0.06509353 -0.09965827  0.037111\n  -0.22219227  0.138976    0.05387096 -0.16252273  0.0242027  -0.03577265\n   0.09416538  0.02495759  0.08320808 -0.09650289 -0.1836403   0.02171025\n   0.20891494  0.1271028  -0.05591395  0.16737138  0.31216323  0.05845862\n   0.12137465 -0.10933263  0.0466614  -0.16188817 -0.18573259 -0.11670873\n   0.08263651  0.02995065 -0.19853099 -0.02782761 -0.00777264  0.01180664\n   0.17371325 -0.10556499  0.06268381 -0.17680615 -0.21315534 -0.1212349\n   0.15993951 -0.11417753 -0.07202455 -0.08273791  0.17339918 -0.23295996\n   0.08958589  0.09069223 -0.08127915  0.12468246  0.2623122   0.06422477\n   0.25816488 -0.1465417   0.0777773   0.01557305  0.14724407 -0.15491404\n  -0.00156215 -0.22082442 -0.29494876  0.33562806 -0.02730435 -0.00707716\n   0.10039996 -0.02551797 -0.05305411 -0.17007957  0.04528431 -0.0936088\n  -0.30674624  0.18057615 -0.07902205  0.08348475  0.06947035  0.03301615\n  -0.11256561  0.03714008 -0.01555683  0.11608716 -0.05519899 -0.23821025\n   0.02735442  0.19606116  0.02084648  0.41384423 -0.07576811 -0.10238604\n  -0.0380757   0.1923537  -0.25028428  0.04975518 -0.07473037  0.07072806\n   0.08671676  0.08983836  0.11755912 -0.37322432 -0.09818657 -0.07177074\n  -0.11857498 -0.22580905 -0.1524703  -0.10811641  0.03113354  0.08470914\n  -0.00527118 -0.30734542  0.11211956  0.17210214 -0.18590069 -0.253243\n   0.07904629 -0.00438886 -0.17456537 -0.18251613 -0.38141814 -0.23924062\n  -0.17819849  0.15044393 -0.31227627 -0.3872469   0.03711067 -0.09579282\n  -0.01034652 -0.11039068 -0.00802334  0.04774079  0.01811191  0.07211188\n  -0.03000485  0.06863642 -0.03969487  0.18527721  0.17767742 -0.18251541\n   0.1175087   0.0215897  -0.17009301 -0.08570173 -0.01107419 -0.18877774\n  -0.19360887 -0.21771581  0.27250564 -0.07355751  0.1843741   0.1025189\n   0.24258198  0.24938945 -0.12068594 -0.21874762  0.01015048 -0.07270237\n  -0.16972996 -0.08136529 -0.17065531 -0.03039329  0.07684421 -0.12473846\n  -0.17573121 -0.21012329  0.09477858 -0.16758087  0.14222544  0.0115684\n   0.13548712 -0.04835992 -0.047315   -0.06056635 -0.17963372 -0.02778357\n  -0.13107796 -0.34027866  0.11040347  0.00519786  0.06334054 -0.10062409\n   0.07201411  0.09378247 -0.03423337 -0.2052359  -0.03419803 -0.13837542\n   0.25130758 -0.2588583  -0.15555257  0.11593471 -0.06936386  0.2143079\n   0.094049   -0.07625496 -0.02540628 -0.02718197 -0.16891208  0.10747167\n   0.17426172 -0.042935    0.0086761  -0.14430082  0.00305612  0.05286587\n   0.00360353  0.02559794 -0.10137238 -0.00650282 -0.1395044  -0.1105178\n  -0.0202186   0.06734218 -0.14005834  0.11571112 -0.07422087  0.03752301\n   0.03282454  0.23002025  0.09906279 -0.14283791  0.17496227  0.0872646\n   0.13755974  0.00098384  0.07986115 -0.0965929   0.13335638 -0.17461698\n  -0.23654918  0.05385043 -0.00666071 -0.00206225  0.00949342  0.16272323\n   0.18375488 -0.03237211  0.1061708  -0.11194807 -0.1031012  -0.22267263\n   0.00279301 -0.02151739  0.20859419 -0.03460366 -0.0960272  -0.19117026\n  -0.07890052  0.09048937 -0.068363    0.2441441   0.06396291 -0.11488332\n  -0.02225552 -0.16457812  0.22170599  0.02902457 -0.17392607  0.02154004\n   0.07660242 -0.09943686 -0.0478159   0.01306699 -0.14820224 -0.23363522\n  -0.00246772 -0.07859506  0.21360904 -0.21283834 -0.230838    0.10115786\n   0.16966553  0.22535986  0.16098318  0.01356226  0.14119722 -0.06337845\n  -0.06527208 -0.06198015 -0.12820417  0.05815347 -0.22353144 -0.13764638\n   0.04808526 -0.01060772 -0.09646636 -0.01569335 -0.1375433   0.09438498\n  -0.06111199 -0.15005317  0.05377191  0.0659612   0.10152406 -0.30666453\n  -0.19555168 -0.06605764  0.00812249 -0.01276721 -0.00849926 -0.16216823\n  -0.10879259 -0.01578609 -0.0353886  -0.02269049 -0.08521605  0.03787727\n   0.00092221 -0.04287263 -0.22845241 -0.0952947  -0.09093014  0.04400957\n   0.01358028  0.23305963 -0.24197996  0.06025798  0.19987068 -0.13480957\n  -0.126718   -0.01720633  0.06335342 -0.12703483  0.07825837  0.08627523\n  -0.02975255 -0.16438589  0.08981166  0.02609529 -0.03161672  0.14790055\n  -0.09943186  0.17525947 -0.01375055  0.23164546 -0.20478159  0.02583731\n  -0.2054277  -0.12760355  0.15142365 -0.02304077  0.02565055 -0.01815061\n  -0.02359406  0.18152502 -0.07347564 -0.16628493  0.15017375 -0.1893321\n  -0.14752416  0.05731994  0.26899898 -0.1306104  -0.06425247 -0.34322542\n  -0.12789103 -0.02584418  0.22317213 -0.04171167  0.2168753  -0.178269\n  -0.0176105  -0.09029625 -0.02549393  0.11204056  0.00391221  0.09883584\n   0.04617636  0.01109007 -0.19997197 -0.01675315  0.07021809  0.2402693\n   0.08051603 -0.07185374 -0.23739132 -0.19452268 -0.19989578  0.17249994]]\n","output_type":"stream"}]},{"cell_type":"code","source":"vector.shape","metadata":{"execution":{"iopub.status.busy":"2024-09-29T14:16:34.436553Z","iopub.execute_input":"2024-09-29T14:16:34.437205Z","iopub.status.idle":"2024-09-29T14:16:34.443569Z","shell.execute_reply.started":"2024-09-29T14:16:34.437165Z","shell.execute_reply":"2024-09-29T14:16:34.442668Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"(1, 768)"},"metadata":{}}]},{"cell_type":"code","source":"pip install einops","metadata":{"execution":{"iopub.status.busy":"2024-09-29T15:04:09.912449Z","iopub.execute_input":"2024-09-29T15:04:09.912829Z","iopub.status.idle":"2024-09-29T15:04:22.117225Z","shell.execute_reply.started":"2024-09-29T15:04:09.912792Z","shell.execute_reply":"2024-09-29T15:04:22.116198Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Collecting einops\n  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\nDownloading einops-0.8.0-py3-none-any.whl (43 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: einops\nSuccessfully installed einops-0.8.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nfrom transformers import AutoTokenizer, AutoModel, AutoImageProcessor\nfrom PIL import Image\nimport requests\n\nprocessor = AutoImageProcessor.from_pretrained(\"nomic-ai/nomic-embed-vision-v1.5\")\nvision_model = AutoModel.from_pretrained(\"nomic-ai/nomic-embed-vision-v1.5\", trust_remote_code=True)\n\nimage = Image.open(\"/kaggle/input/images2/IMG1.jpg\")\n\ninputs = processor(image, return_tensors=\"pt\")\n\nimg_emb = vision_model(**inputs).last_hidden_state\nimg_embeddings = F.normalize(img_emb[:, 0], p=2, dim=1)\nprint(img_embeddings)","metadata":{"execution":{"iopub.status.busy":"2024-09-29T15:09:42.425653Z","iopub.execute_input":"2024-09-29T15:09:42.426047Z","iopub.status.idle":"2024-09-29T15:09:43.224959Z","shell.execute_reply.started":"2024-09-29T15:09:42.426008Z","shell.execute_reply":"2024-09-29T15:09:43.223977Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"tensor([[-4.5462e-02, -6.6609e-03, -1.7633e-02, -5.9355e-02, -5.1656e-02,\n         -7.1600e-04, -2.1137e-02, -6.4139e-02, -2.4770e-02, -2.1499e-02,\n         -2.0925e-02, -1.9043e-02,  1.3856e-02, -1.0005e-01, -4.6762e-02,\n          1.4080e-02,  3.2178e-02, -1.2627e-02,  6.2626e-02, -7.4154e-02,\n         -3.8940e-02,  1.4525e-02,  1.0743e-02, -2.0500e-02, -8.4310e-03,\n         -5.2803e-02,  1.0965e-02, -5.5497e-02, -5.1630e-02, -3.1386e-02,\n         -1.5890e-02, -3.2528e-02, -4.3779e-02, -3.6613e-03, -4.8817e-02,\n          2.6369e-04, -4.1981e-02, -5.6133e-04, -3.7271e-02, -3.6555e-02,\n         -4.4310e-02, -6.2193e-02, -6.1359e-02, -9.4624e-03, -2.6119e-02,\n          2.6254e-02, -4.0834e-02, -1.6405e-02, -1.8041e-02, -3.9325e-02,\n         -5.4687e-02, -5.9463e-02, -1.6678e-03, -5.3135e-02, -5.5968e-02,\n         -4.4107e-02, -6.1492e-02,  1.2281e-02,  1.8484e-02, -1.4581e-03,\n         -4.8707e-02, -1.2518e-02, -2.2789e-02, -1.1079e-02, -2.1965e-02,\n          1.7827e-02, -4.7583e-02, -5.6914e-02, -3.4345e-02, -2.7157e-02,\n         -2.5459e-02, -2.4931e-02, -3.5720e-03, -3.9206e-02, -1.1374e-02,\n         -2.3407e-02, -4.0221e-02, -1.7359e-02, -6.4690e-03, -2.0719e-02,\n         -7.0155e-02, -7.3028e-03, -2.9961e-02,  2.2414e-02, -3.7724e-02,\n         -3.8685e-02, -1.0177e-02, -5.8314e-02, -1.5176e-02, -3.0586e-02,\n         -3.1822e-02, -4.0904e-02, -5.7220e-02, -3.4823e-02,  2.1564e-03,\n         -6.6947e-02, -5.3425e-02, -3.0463e-03, -3.8608e-02, -6.8598e-02,\n         -2.0793e-02, -2.3531e-02, -2.0693e-02,  4.8381e-04, -2.0896e-02,\n         -3.9667e-02, -6.1400e-02, -4.3617e-02, -5.3175e-02, -5.1724e-02,\n         -1.0972e-02, -4.6647e-02, -1.9127e-02, -6.5650e-02, -1.5089e-02,\n         -4.1734e-02,  2.5827e-02, -4.3811e-02, -8.3837e-03, -2.0534e-03,\n         -2.7000e-02, -1.8802e-02, -7.3267e-02, -4.9278e-02, -2.6625e-02,\n         -2.6015e-02,  2.1397e-02, -4.5024e-02,  4.7234e-03, -1.2131e-02,\n         -1.6699e-02,  4.2531e-02, -1.1099e-02,  2.1372e-02, -3.6280e-02,\n         -5.2408e-02, -4.6106e-02, -5.2658e-03, -3.6600e-02, -6.4016e-02,\n         -1.8508e-02, -5.9850e-02, -1.2209e-02, -3.8371e-02, -2.6878e-02,\n         -2.2405e-02, -7.3808e-02,  1.2788e-02, -2.0556e-02, -3.1411e-02,\n          1.0653e-03, -2.1259e-02, -6.2644e-04,  8.2795e-03, -6.5966e-03,\n         -6.2587e-02, -6.9124e-02,  3.3586e-03, -9.4365e-02, -2.7970e-02,\n          6.5505e-04, -1.7675e-03, -5.0933e-02, -3.4793e-02, -5.4147e-02,\n         -5.2476e-02, -2.5395e-02, -2.0162e-02, -4.4657e-02, -6.1450e-03,\n         -5.0807e-02, -1.0932e-02, -5.5162e-02, -3.2710e-02, -2.3700e-02,\n          1.7703e-02, -4.7111e-02, -4.6969e-02, -1.5181e-02, -7.5711e-02,\n         -7.5465e-03, -5.9391e-02, -6.0614e-02, -6.1974e-02, -2.4797e-03,\n         -2.3560e-02, -5.1806e-02, -3.7538e-03, -9.1197e-03,  2.3112e-02,\n          1.2266e-02, -4.2026e-02, -4.8399e-02, -4.4136e-02, -3.3128e-02,\n          3.2481e-02, -5.3093e-02, -6.4930e-03, -4.7273e-02, -9.5557e-02,\n         -2.9422e-02, -1.2833e-02, -4.4785e-02, -5.6730e-03, -1.4243e-02,\n         -5.5687e-02, -2.8499e-02, -1.5103e-02, -5.9644e-02, -1.3795e-02,\n         -3.4881e-02,  2.7747e-02, -3.3640e-05, -3.1398e-02, -6.6522e-02,\n         -4.7316e-02, -2.3419e-02,  1.0163e-02,  1.8626e-02,  4.3299e-03,\n         -3.7524e-02,  1.0663e-04, -6.6022e-03,  3.8545e-03, -3.2465e-02,\n         -3.8946e-02, -2.2756e-02, -1.0308e-02,  5.0353e-03, -2.2961e-02,\n         -7.8966e-03, -1.4742e-02,  8.6244e-04, -1.6289e-03, -5.4751e-02,\n         -9.8814e-03,  6.3648e-03, -6.1460e-02, -5.6516e-02, -7.0333e-04,\n         -3.1439e-02, -1.7163e-03, -3.1775e-02, -2.0038e-03,  4.2479e-03,\n         -3.4099e-02, -4.3492e-02, -5.0514e-02, -4.5215e-02, -2.0152e-02,\n         -2.1197e-02, -1.2749e-02,  1.2743e-02, -3.1627e-02,  6.2420e-03,\n         -4.5118e-02, -5.7119e-02, -3.0557e-02, -3.5903e-02, -3.5017e-02,\n         -1.7992e-02,  3.6623e-03, -1.7779e-02, -1.4840e-02, -1.6354e-02,\n         -3.6664e-02, -5.0343e-02, -1.4419e-02,  2.3570e-03, -2.2798e-02,\n         -2.2757e-02,  1.6774e-02,  3.8813e-03,  1.6671e-04, -1.4733e-02,\n         -4.3790e-02,  4.0907e-03, -2.5532e-02, -3.0424e-02, -2.2295e-02,\n         -5.6513e-02, -3.5331e-02,  1.3717e-03, -1.4858e-02, -3.6588e-02,\n         -1.8784e-02, -3.5744e-02, -3.8598e-02, -3.8558e-02, -4.4447e-02,\n         -1.0780e-02, -1.7905e-02, -6.6801e-04, -3.2296e-02, -2.3404e-02,\n         -2.3148e-02, -1.1452e-02, -4.7552e-02, -6.5070e-02, -3.2920e-02,\n         -3.7108e-02, -3.9304e-02,  2.7644e-02, -7.2997e-03, -9.3286e-03,\n         -6.5511e-02, -4.7760e-02, -1.9256e-02, -2.5288e-03, -2.0936e-02,\n         -3.8035e-02, -3.6441e-02, -2.2776e-02, -9.4849e-03, -5.3892e-02,\n         -6.3517e-02,  1.8096e-02,  1.0792e-02, -3.1881e-03, -3.3757e-02,\n         -1.1154e-02, -1.1783e-02, -7.6626e-03, -2.9515e-02, -1.2202e-02,\n         -3.8095e-02, -3.9267e-03,  2.6411e-02, -3.6025e-03,  2.3114e-02,\n         -7.1099e-02, -1.8401e-02, -2.5694e-02, -4.0281e-02,  3.9050e-03,\n         -1.7543e-02, -1.5912e-02, -3.0424e-02, -6.4967e-02, -1.7473e-02,\n         -3.5705e-02, -1.4512e-02, -1.5722e-02, -1.0232e-01, -6.1931e-02,\n         -3.5247e-02, -2.9520e-02, -3.5569e-02, -2.1414e-03, -1.5305e-02,\n         -6.4865e-02, -3.7217e-02, -3.0071e-02, -4.8916e-02, -4.5181e-02,\n         -1.7377e-02, -5.3904e-02, -6.3260e-03, -4.6559e-02, -9.6500e-03,\n          1.9259e-03, -4.8623e-02, -1.5103e-02, -7.0245e-03, -6.1105e-02,\n         -2.6551e-02, -3.4327e-02, -5.6890e-02,  5.3676e-03, -4.6810e-02,\n         -5.3572e-03,  7.6572e-03, -3.1414e-02, -1.2917e-02, -4.4497e-02,\n          1.4728e-03, -3.1695e-02, -2.3897e-02, -4.0582e-02, -2.8745e-02,\n         -4.2399e-02, -3.1056e-02, -3.7734e-02, -2.3736e-02, -3.3832e-03,\n         -4.3807e-02, -1.8628e-02, -2.6615e-02, -5.5970e-02, -1.4763e-02,\n         -5.9645e-03, -3.0826e-02, -7.1987e-02, -2.1223e-02,  2.7094e-03,\n          3.3582e-02, -9.0215e-03, -3.2113e-02, -2.9958e-02,  3.2085e-02,\n         -1.3073e-02, -6.0898e-02, -7.2268e-03, -4.4780e-02, -1.5071e-02,\n         -3.6136e-02, -2.4804e-02, -4.7150e-02, -2.7355e-02, -6.3872e-03,\n          1.7525e-02,  1.3256e-02, -5.4325e-02, -3.2024e-02, -3.8500e-02,\n         -2.0801e-02, -2.7928e-02, -2.8767e-02, -2.0657e-02, -2.4282e-02,\n         -2.6773e-02, -5.5464e-02, -8.7989e-03, -3.0572e-02, -2.1653e-02,\n          5.2350e-03, -1.5306e-02, -3.9944e-02, -1.4836e-02, -4.2358e-02,\n         -2.5341e-02, -5.0410e-02, -5.4870e-02, -6.4359e-02, -3.0690e-02,\n         -6.1033e-02, -5.1526e-02, -6.3598e-02, -3.5596e-02, -4.1598e-02,\n         -5.4594e-02, -6.1931e-02, -6.9480e-02,  5.7079e-03, -9.9350e-04,\n         -2.4379e-02, -4.2974e-02,  1.9106e-02,  2.1992e-02,  1.5116e-04,\n         -2.8573e-02, -3.5917e-02, -6.7888e-02, -1.0755e-02,  1.6595e-02,\n         -4.6304e-02, -1.4915e-02,  2.3448e-02, -5.4816e-03, -4.5523e-02,\n         -1.6533e-02, -4.6071e-02, -4.5112e-02, -4.3712e-02, -3.8170e-02,\n         -5.6230e-02, -2.4653e-02, -9.3799e-02, -5.0946e-02, -2.1416e-02,\n         -3.6680e-02,  3.0317e-03,  1.7447e-02, -7.1548e-02, -1.9242e-02,\n         -5.3820e-02, -5.1755e-02, -4.6984e-02,  1.2481e-02, -1.0776e-02,\n          4.8593e-03, -4.1773e-02, -5.5801e-02, -1.8858e-02, -2.2930e-02,\n         -5.0172e-03, -5.5324e-03, -2.6431e-02, -2.2454e-02, -5.9961e-02,\n          3.6230e-02, -2.8400e-02,  5.7687e-04, -2.2698e-02, -6.3363e-02,\n         -6.0930e-02, -4.5710e-02,  3.3256e-02, -3.7301e-02, -7.2493e-04,\n         -2.4943e-02,  9.2285e-03, -1.5542e-02, -1.9576e-02, -3.3002e-02,\n         -1.2247e-02, -8.2709e-03, -5.1413e-02, -4.1356e-02, -3.9732e-02,\n         -2.9536e-02,  3.7730e-03, -2.5860e-02,  1.8095e-04,  8.1741e-03,\n         -2.1192e-02, -4.9339e-02,  1.9512e-03,  3.6637e-04, -3.6331e-02,\n          6.8624e-03, -5.0302e-03,  1.9640e-02, -8.8087e-03, -2.3716e-02,\n         -2.4952e-02, -2.5069e-02, -2.6250e-04, -2.0607e-02, -5.2621e-02,\n         -2.7511e-02, -8.5876e-02, -6.3994e-02, -3.9726e-02,  5.2585e-03,\n          2.4927e-03,  5.3043e-03, -5.8996e-02, -5.0033e-02, -1.4050e-02,\n         -5.6962e-02, -5.0091e-02,  1.8294e-02, -5.8473e-02, -2.2698e-02,\n         -5.4877e-03, -7.0238e-02, -3.5684e-02, -1.5777e-02,  3.6367e-02,\n         -6.9471e-02, -4.4082e-02, -1.0296e-02, -3.3659e-02, -1.6260e-02,\n          7.6521e-03, -2.2792e-02, -3.3296e-02, -1.4645e-02, -7.2207e-02,\n          1.4263e-03, -1.8547e-02, -3.7476e-03, -2.9337e-02,  1.3126e-02,\n         -5.6669e-02,  3.5944e-02, -5.9220e-02,  6.6398e-03, -4.5706e-02,\n         -3.8980e-02, -7.7425e-04, -3.3970e-04,  1.1181e-02, -1.1598e-02,\n         -7.4032e-02, -3.1066e-02, -2.2787e-02, -1.2780e-02, -2.5880e-02,\n         -6.5029e-02, -6.0281e-02, -4.2679e-02,  2.3584e-02, -1.0402e-02,\n         -1.3523e-02, -4.6647e-02, -2.9798e-02,  2.3495e-02, -2.0171e-02,\n         -2.0078e-02, -2.0087e-02, -4.4844e-02, -3.5305e-02, -1.9406e-02,\n         -4.4895e-02,  1.6226e-02, -3.2920e-02, -4.1525e-03, -2.4861e-02,\n         -1.4875e-02, -2.2292e-02, -4.7381e-03,  7.5686e-03, -3.8243e-02,\n         -2.9895e-02, -7.4979e-03, -1.8794e-02, -2.4253e-02, -3.0295e-02,\n         -3.3607e-03, -5.3480e-02, -2.0127e-02, -1.3312e-02, -3.3571e-03,\n          3.1703e-03, -5.0784e-02, -2.1456e-02,  3.3205e-03, -7.7430e-02,\n         -2.1203e-02, -1.5760e-02,  5.3269e-02,  6.6054e-03, -7.5428e-03,\n         -1.0524e-02, -2.4680e-02, -1.6652e-03,  5.0609e-03,  1.2807e-02,\n         -1.7527e-02, -2.5992e-02, -1.7329e-02, -3.7762e-02,  1.7541e-02,\n          2.6896e-02, -6.3777e-02, -6.3353e-02, -6.6776e-02, -4.1049e-02,\n          2.2579e-02, -8.7682e-04, -9.5254e-03, -5.7707e-02, -1.4219e-02,\n         -4.0062e-02, -4.7899e-02, -2.4896e-02, -1.5148e-02,  1.4243e-04,\n         -8.3569e-03,  3.3665e-02, -2.1759e-02, -1.8677e-02, -6.3594e-02,\n         -3.8712e-02, -4.7873e-02, -2.4924e-02, -4.9529e-02, -1.5632e-02,\n         -2.9317e-02, -4.8353e-02, -3.5335e-02, -4.1901e-02, -2.5579e-02,\n         -1.8460e-02,  1.2554e-02,  2.3524e-02, -2.0066e-02,  1.9697e-02,\n          5.8965e-02, -5.1773e-02, -3.7493e-02, -4.2057e-02, -3.4874e-02,\n          1.4333e-02, -5.1646e-02,  5.3306e-03, -1.6333e-03, -5.4650e-02,\n         -6.0946e-02,  6.5578e-03,  3.8795e-02, -4.9997e-02,  1.2546e-02,\n         -7.0838e-02, -4.0773e-02, -5.3845e-02, -4.6340e-02, -4.5809e-02,\n         -5.3989e-02, -3.0909e-02, -8.9654e-02, -6.0478e-02,  1.1815e-02,\n          2.2671e-02, -1.9053e-02, -6.4758e-02, -1.9819e-02, -1.6633e-02,\n         -2.7682e-02, -1.5674e-02, -3.3445e-02, -3.1064e-02, -5.4302e-02,\n         -1.1263e-02, -2.7020e-02, -1.0725e-02,  4.3494e-02, -2.1267e-02,\n          1.0746e-02, -4.7453e-02, -8.1324e-02, -6.7945e-03, -3.2471e-02,\n         -1.0288e-02, -4.9807e-02, -5.1885e-02, -6.8608e-03,  6.0893e-04,\n         -3.9998e-02, -5.5399e-02, -7.7477e-02, -1.6765e-02, -4.7202e-02,\n         -6.7080e-03, -4.6082e-02, -5.4481e-02, -2.2816e-02, -1.6100e-02,\n         -3.8448e-02, -3.6258e-02,  7.4513e-03, -3.1018e-02, -1.7996e-02,\n         -2.3356e-02,  1.5103e-02, -5.3265e-02, -1.0278e-02, -4.9943e-02,\n         -7.5877e-02, -6.7233e-02, -2.0926e-02, -3.8140e-02, -2.7920e-02,\n         -1.5365e-02, -8.7572e-03, -4.7969e-03, -2.3303e-02, -7.3498e-03,\n         -1.4772e-02, -5.1119e-04, -5.7137e-03, -6.0342e-02, -8.3569e-03,\n         -4.3088e-02, -2.6265e-02, -1.9820e-02, -4.1719e-02, -1.4264e-02,\n         -5.9912e-02, -4.9450e-02, -3.6320e-02, -4.7649e-02, -4.6420e-02,\n         -7.8246e-02, -1.6540e-02,  2.6460e-03]], grad_fn=<DivBackward0>)\n","output_type":"stream"}]},{"cell_type":"code","source":"img_embeddings.shape","metadata":{"execution":{"iopub.status.busy":"2024-09-29T15:09:51.928694Z","iopub.execute_input":"2024-09-29T15:09:51.929731Z","iopub.status.idle":"2024-09-29T15:09:51.935584Z","shell.execute_reply.started":"2024-09-29T15:09:51.929685Z","shell.execute_reply":"2024-09-29T15:09:51.934579Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"torch.Size([1, 768])"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}