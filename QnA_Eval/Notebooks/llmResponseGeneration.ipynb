{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "import os\n",
    "from AnswerGeneration.getAnswer import generate_answer_withContext\n",
    "\n",
    "def load_json(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def getRanking(file_path, query_id):\n",
    "    ranking = load_json(file_path)\n",
    "    # print(f\"Ranking from file {file_path}: {ranking}\")\n",
    "    for rank in ranking:\n",
    "        if query_id in rank:\n",
    "            return rank[query_id][0]\n",
    "    print(\"Query ID not found\")\n",
    "    return None\n",
    "\n",
    "def getDocument(file_path, context_document_id):\n",
    "    documents = load_json(file_path)\n",
    "    \n",
    "    found = 0\n",
    "    context_document_text = {}\n",
    "    # Iterate over documents and check if any document matches the wikipedia_id\n",
    "    for doc in documents:\n",
    "        # Iterate through context_document_id to find the corresponding key (query_id)\n",
    "        for query_id, wiki_id in context_document_id.items():\n",
    "            # Check if the wikipedia_id matches the value (wiki_id)\n",
    "            if wiki_id == doc[\"wikipedia_id\"]:\n",
    "                # Update the query_id with the joined text of the document\n",
    "                context_document_text[query_id] = \" \".join(doc[\"text\"])  \n",
    "                found += 1\n",
    "\n",
    "    print(f\"Found {found} documents\")\n",
    "    return context_document_text\n",
    "\n",
    "query_ids = load_json(\"QnA_Eval_Query_ids.json\")\n",
    "query = load_json(\"../Datasets/FinalDataset.json\")\n",
    "\n",
    "\n",
    "files = [\"tf-idf_1_0.json\"]\n",
    "\n",
    "for file in files:\n",
    "    results = []\n",
    "    context_document_id = {}\n",
    "    for query_id in query_ids:\n",
    "        context_document_id[query_id] = getRanking(file, query_id)\n",
    "\n",
    "    miniWikiCollection_path = \"../Datasets/mini_wiki_collection.json\"\n",
    "    context_documents = getDocument(miniWikiCollection_path, context_document_id)\n",
    "\n",
    "    for query_id in query_ids:\n",
    "        context_text = context_documents[query_id]\n",
    "        query_text = query[query_id][\"input\"]\n",
    "        answer_text = list(query[query_id][\"output\"].values())[0]\n",
    "        response = generate_answer_withContext(query_text, context_text)\n",
    "        results.append({\n",
    "            \"query_id\": query_id,\n",
    "            \"top_document_id\": context_document_id[query_id],  \n",
    "            \"input\" : query_text,\n",
    "            \"response\": response,\n",
    "            \"gold_answer\": answer_text\n",
    "        })\n",
    "        print(results)\n",
    "    break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
